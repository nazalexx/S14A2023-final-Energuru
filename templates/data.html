{% extends "index.html" %}

{% block title %}Data{% endblock %}

{% block content %}
<style>
    h1 {
        margin-bottom: 1rem;
    }
</style>
    <main class="container mt-4">
        <div class="bg-body-tertiary p-5 rounded mt-2">
            <h1 id="Information">Information</h1>
            <p class="lead">The National Renewable Energy Laboratory (NREL) and its research 
                partners <a href="https://www.nrel.gov/buildings/end-use-load-profiles.html">have developed</a> a database of end-use load 
                profiles (EULP) representing all major end uses, building types, and climate regions in the U.S. commercial and residential 
                building stock. The particular database <code>baseline_metadata_and_annual_results.csv</code> (1.5 GB) with approximately 
                550,000 residential buildings nationwide can be downloaded 
                <a href="https://data.openei.org/s3_viewer?bucket=oedi-data-lake&prefix=nrel-pds-building-stock%2Fend-use-load-profiles-for-us-building-stock%2F2022%2Fresstock_amy2018_release_1%2Fmetadata_and_annual_results%2Fnational%2Fcsv%2F">here</a>.</p>
        </div>

        <div class="bg-body-tertiary p-5 rounded mt-2">
            <h1 id="DataCleaning">Data Cleaning</h1>
            <p class="lead">The (reproducible) data cleaning process is documented in <code>model/data_cleaning.ipynb</code> 
                <a href="model/data_cleaning.ipynb">here</a>. The notebook was written as part of the final project for CSCI E-109A 
                Introduction to Data Science at Harvard Extension School by Aleksejs Nazarovs, Ari Neumann, Melina Sotiriou Droz, and Sergei 
                Zhits in 2022. It goes over each column of the raw dataset, deletes columns with no information and those irrelevant to (total) 
                energy consumption prediction as well as tries to eliminate variables which essentially contain the same information as some other
                ones. The notebook also makes sure all the variables are assigned the simplest possible data types. Finally, it converts 
                categorical variables into binary dummy variables. The only change that was made to this notebook as part of this CSCI S-14A 
                project was that the process of dummy variable creation does not exclude the first category anymore. It simplifies user inputs 
                conversion into model inputs but does not cause overfitting since the only model class used for this project is gradient boosting. 
                The cleaned dataset is stored in <code>data/energy_cleaned.parquet</code> <a href="data/energy_cleaned.parquet">here</a>.</p>
        </div>

        <div class="bg-body-tertiary p-5 rounded mt-2">
            <h1 id="MachineLearning">Machine Learning</h1>
            <p class="lead">The process documented in <code>model/model.ipynb</code> <a href="model/model.ipynb">here</a> contains some 
                additional minor data cleaning as well as converts the data file from csv to parquet format, for faster reading efficiency 
                and ability to upload to GitHub. The notebook then defines changeable and unchangeable features, which were determined via 
                a thorough and extensive data exploration. The column titles are then reformatted for better readability on the website. Some 
                code elements from the aforementioned project by Nazarovs et al. were borrowed to train the model yet the model itself (namely, 
                histogram-based gradient boosting regressor) was tuned and trained on the updated data. The model was saved as a joblib file, 
                to use it from within the app. The notebook then generates three mock inputs (to be offered to fill out the form by on the website): 
                one for "inefficient", one for "average" and one for "efficient" dwelling unit in terms of energy consumption. The notebook also
                defines a function to convert user inputs (dictionary without dummy variables) into model input (pandas.DataFrame with dummy variables)
                as well as a function to generate prediction and biggest possible consumption reductions by feature. Finally, the notebook saves 
                the necessary data (changeable features, feature names and possible values, the three inputs, etc.) into json files.</p>
        </div>
    </main>
{% endblock %}
